{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Long-Term Memory from a Conversation\n",
    "This Notebook shows how to set up an agent that can extract memories from a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain==0.1.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.1.6)\n",
      "Requirement already satisfied: langchain_openai==0.0.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.0.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.12.0) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (3.11.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (0.0.20)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (0.1.23)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain==0.1.6) (8.5.0)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain_openai==0.0.5) (0.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==1.12.0) (3.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.12.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.12.0) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.6) (3.0.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.6) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.6) (2.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken<0.6.0,>=0.5.2->langchain_openai==0.0.5) (2024.11.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai==1.12.0 langchain==0.1.6 langchain_openai==0.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set model variables\n",
    "# OPENAI_BASE_URL = \"https://api.openai.com/v1\"\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# OPENAI_ORGANIZATION = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "\n",
    "# Initialize LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Demos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craete custom LLM class for interacting with locally running deepseek on ollama server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "import requests\n",
    "\n",
    "# Define the custom OllamaLLM class\n",
    "class OllamaLLM:\n",
    "    # Using the distilled model with 8B parameters\n",
    "    model: str = \"deepseek-r1:8b\"\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        url = \"http://localhost:11434/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        response = requests.post(url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Agent: Memory Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to assess this message to determine if it contains any details about a family's dining habits that are worth recording for our knowledge base. The categories we're interested in are food allergies, foods they like, foods they dislike, and attributes that impact meal planning.\n",
      "\n",
      "Looking at the message: \"I despite eggplants please dont even mention the word eggplant.\" The user is expressing a dislike for eggplants. So this directly falls under category 3, which is foods they dislike. Even though it's a strong dislike (\"despite\"), it's clear information about what they don't eat. \n",
      "\n",
      "So, I should check if there are any other details that might fit into the other categories. The message doesn't mention anything about allergies, food preferences beyond eggplants, or attributes like location or family members. It's just focused on disliking a specific food.\n",
      "\n",
      "Therefore, this message does provide valuable information by indicating a disliked food item, so I should return TRUE.\n",
      "</think>\n",
      "\n",
      "TRUE\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "system_prompt_initial = \"\"\"\n",
    "Your job is to assess a brief chat history in order to determine if the conversation contains any details about a family's dining habits. \n",
    "\n",
    "You are part of a team building a knowledge base regarding a family's dining habits to assist in highly customized meal planning.\n",
    "\n",
    "You play the critical role of assessing the message to determine if it contains any information worth recording in the knowledge base.\n",
    "\n",
    "You are only interested in the following categories of information:\n",
    "\n",
    "1. The family's food allergies (e.g. a dairy or soy allergy)\n",
    "2. Foods the family likes (e.g. likes pasta)\n",
    "3. Foods the family dislikes (e.g. doesn't eat mussels)\n",
    "4. Attributes about the family that may impact weekly meal planning (e.g. lives in Austin; has a husband and 2 children; has a garden; likes big lunches; etc.)\n",
    "\n",
    "When you receive a message, you perform a sequence of steps consisting of:\n",
    "\n",
    "1. Analyze the message for information.\n",
    "2. If it has any information worth recording, return TRUE. If not, return FALSE.\n",
    "\n",
    "You should ONLY RESPOND WITH TRUE OR FALSE. Absolutely no other information should be provided.\n",
    "\n",
    "Take a deep breath, think step by step, and then analyze the following message:\n",
    "\"\"\"\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_initial),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Remember, only respond with TRUE or FALSE. Do not provide any other information.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = OllamaLLM()\n",
    "\n",
    "# Define a function to convert dictionaries to LangChain message objects\n",
    "def convert_to_messages(messages):\n",
    "    langchain_messages = []\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content=message[\"content\"]))\n",
    "        elif message[\"role\"] == \"system\":\n",
    "            langchain_messages.append(SystemMessage(content=message[\"content\"]))\n",
    "    return langchain_messages\n",
    "\n",
    "# Define a function to format the prompt\n",
    "def format_prompt(messages):\n",
    "    # Convert the input messages to LangChain message objects\n",
    "    langchain_messages = convert_to_messages(messages)\n",
    "    \n",
    "    # Format the messages into a ChatPromptValue object\n",
    "    chat_prompt_value = prompt.format_messages(messages=langchain_messages)\n",
    "    \n",
    "    # Convert the ChatPromptValue object into a single string\n",
    "    formatted_prompt = \"\\n\".join([message.content for message in chat_prompt_value])\n",
    "    return formatted_prompt\n",
    "\n",
    "# Define a function to run the pipeline\n",
    "def run_pipeline(messages):\n",
    "    # Format the prompt\n",
    "    formatted_prompt = format_prompt(messages)\n",
    "    \n",
    "    # Call the OllamaLLM\n",
    "    response = llm._call(formatted_prompt)\n",
    "    return response\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I despite eggplants please dont even mention the word eggplant\"}\n",
    "]\n",
    "\n",
    "# Run the pipeline\n",
    "response = run_pipeline(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Agent: Memory Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import StructuredTool\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Category(str, Enum):\n",
    "    Food_Allergy = \"Allergy\"\n",
    "    Food_Like = \"Like\"\n",
    "    Food_Dislike = \"Dislike\"\n",
    "    Family_Attribute = \"Attribute\"\n",
    "\n",
    "\n",
    "class Action(str, Enum):\n",
    "    Create = \"Create\"\n",
    "    Update = \"Update\"\n",
    "    Delete = \"Delete\"\n",
    "\n",
    "\n",
    "class AddKnowledge(BaseModel):\n",
    "    knowledge: str = Field(\n",
    "        ...,\n",
    "        description=\"Condensed bit of knowledge to be saved for future reference in the format: [person(s) this is relevant to] [fact to store] (e.g. Husband doesn't like tuna; I am allergic to shellfish; etc)\",\n",
    "    )\n",
    "    knowledge_old: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"If updating or deleting record, the complete, exact phrase that needs to be modified\",\n",
    "    )\n",
    "    category: Category = Field(\n",
    "        ..., description=\"Category that this knowledge belongs to\"\n",
    "    )\n",
    "    action: Action = Field(\n",
    "        ...,\n",
    "        description=\"Whether this knowledge is adding a new record, updating a record, or deleting a record\",\n",
    "    )\n",
    "\n",
    "\n",
    "def modify_knowledge(\n",
    "    knowledge: str,\n",
    "    category: str,\n",
    "    action: str,\n",
    "    knowledge_old: str = \"\",\n",
    ") -> dict:\n",
    "    print(\"Modifying Knowledge: \", knowledge, knowledge_old, category, action)\n",
    "    return \"Modified Knowledge\"\n",
    "\n",
    "\n",
    "tool_modify_knowledge = StructuredTool.from_function(\n",
    "    func=modify_knowledge,\n",
    "    name=\"Knowledge_Modifier\",\n",
    "    description=\"Add, update, or delete a bit of knowledge\",\n",
    "    args_schema=AddKnowledge,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the tools to execute them from the graph\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "# Set up the agent's tools\n",
    "agent_tools = [tool_modify_knowledge]\n",
    "\n",
    "tool_executor = ToolExecutor(agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "system_prompt_initial = \"\"\"\n",
    "You are a supervisor managing a team of knowledge eperts.\n",
    "\n",
    "Your team's job is to create a perfect knowledge base about a family's dining habits to assist in highly customized meal planning.\n",
    "\n",
    "The knowledge base should ultimately consist of many discrete pieces of information that add up to a rich persona (e.g. I like pasta; I am allergic to shellfish; I don't eat mussels; I live in Austin, Texas; I have a husband and 2 children aged 5 and 7).\n",
    "\n",
    "Every time you receive a message, you will evaluate if it has any information worth recording in the knowledge base.\n",
    "\n",
    "A message may contain multiple pieces of information that should be saved separately.\n",
    "\n",
    "You are only interested in the following categories of information:\n",
    "\n",
    "1. The family's food allergies (e.g. a dairy or soy allergy) - These are important to know because they can be life-threatening. Only log something as an allergy if you are certain it is an allergy and not just a dislike.\n",
    "2. Foods the family likes (e.g. likes pasta) - These are important to know because they can help you plan meals, but are not life-threatening.\n",
    "3. Foods the family dislikes (e.g. doesn't eat mussels or rarely eats beef) - These are important to know because they can help you plan meals, but are not life-threatening.\n",
    "4. Attributes about the family that may impact weekly meal planning (e.g. lives in Austin; has a husband and 2 children; has a garden; likes big lunches, etc.)\n",
    "\n",
    "When you receive a message, you perform a sequence of steps consisting of:\n",
    "\n",
    "1. Analyze the most recent Human message for information. You will see multiple messages for context, but we are only looking for new information in the most recent message.\n",
    "2. Compare this to the knowledge you already have.\n",
    "3. Determine if this is new knowledge, an update to old knowledge that now needs to change, or should result in deleting information that is not correct. It's possible that a food you previously wrote as a dislike might now be a like, or that a family member who previously liked a food now dislikes it - those examples would require an update.\n",
    "\n",
    "Here are the existing bits of information that we have about the family.\n",
    "\n",
    "```\n",
    "{memories}\n",
    "```\n",
    "\n",
    "Call the right tools to save the information, then respond with DONE. If you identiy multiple pieces of information, call everything at once. You only have one chance to call tools.\n",
    "\n",
    "I will tip you $20 if you are perfect, and I will fine you $40 if you miss any important information or change any incorrect information.\n",
    "\n",
    "Take a deep breath, think step by step, and then analyze the following message:\n",
    "\"\"\"\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_initial),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(\n",
    "    # model=\"gpt-3.5-turbo-0125\",\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Create the tools to bind to the model\n",
    "tools = [convert_to_openai_function(t) for t in agent_tools]\n",
    "\n",
    "knowledge_master_runnable = prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The list of previous messages in the conversation\n",
    "    messages: Sequence[BaseMessage]\n",
    "    # The long-term memories to remember\n",
    "    memories: Sequence[str]\n",
    "    # Whether the information is relevant\n",
    "    contains_information: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "\n",
    "\n",
    "def call_sentinel(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = sentinel_runnable.invoke(messages)\n",
    "    return {\"contains_information\": \"TRUE\" in response.content and \"yes\" or \"no\"}\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the knowledge master\n",
    "def call_knowledge_master(state):\n",
    "    messages = state[\"messages\"]\n",
    "    memories = state[\"memories\"]\n",
    "    response = knowledge_master_runnable.invoke(\n",
    "        {\"messages\": messages, \"memories\": memories}\n",
    "    )\n",
    "    return {\"messages\": messages + [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # We know the last message involves at least one tool call\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # We loop through all tool calls and append the message to our message log\n",
    "    for tool_call in last_message.additional_kwargs[\"tool_calls\"]:\n",
    "        action = ToolInvocation(\n",
    "            tool=tool_call[\"function\"][\"name\"],\n",
    "            tool_input=json.loads(tool_call[\"function\"][\"arguments\"]),\n",
    "            id=tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "        # We call the tool_executor and get back a response\n",
    "        response = tool_executor.invoke(action)\n",
    "        # We use the response to create a FunctionMessage\n",
    "        function_message = ToolMessage(\n",
    "            content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        # Add the function message to the list\n",
    "        messages.append(function_message)\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize a new graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Define the two \"Nodes\"\" we will cycle between\n",
    "graph.add_node(\"sentinel\", call_sentinel)\n",
    "graph.add_node(\"knowledge_master\", call_knowledge_master)\n",
    "graph.add_node(\"action\", call_tool)\n",
    "\n",
    "# Define all our Edges\n",
    "\n",
    "# Set the Starting Edge\n",
    "graph.set_entry_point(\"sentinel\")\n",
    "\n",
    "# We now add Conditional Edges\n",
    "graph.add_conditional_edges(\n",
    "    \"sentinel\",\n",
    "    lambda x: x[\"contains_information\"],\n",
    "    {\n",
    "        \"yes\": \"knowledge_master\",\n",
    "        \"no\": END,\n",
    "    },\n",
    ")\n",
    "graph.add_conditional_edges(\n",
    "    \"knowledge_master\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add Normal Edges that should always be called after another\n",
    "graph.add_edge(\"action\", END)\n",
    "\n",
    "# We compile the entire workflow as a runnable\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = \"There are 6 people in my family. My wife doesn't eat meat and my youngest daughter is allergic to dairy.\"\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=message)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'sentinel':\n",
      "---\n",
      "{'contains_information': 'yes'}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'knowledge_master':\n",
      "---\n",
      "{'messages': [HumanMessage(content=\"There are 6 people in my family. My wife doesn't eat meat and my youngest daughter is allergic to dairy.\"), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_N3OUsTB9UNxI6JF2A6AggH4z', 'function': {'arguments': '{\"knowledge\": \"Family consists of 6 people\", \"category\": \"Attribute\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}, {'index': 1, 'id': 'call_UJz5NaecIpxiwAwlODdUgabA', 'function': {'arguments': '{\"knowledge\": \"Wife doesn\\'t eat meat\", \"category\": \"Dislike\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}, {'index': 2, 'id': 'call_6fnDlpEdBFTgCPBwFISZdXDY', 'function': {'arguments': '{\"knowledge\": \"Youngest daughter is allergic to dairy\", \"category\": \"Allergy\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}]})]}\n",
      "\n",
      "---\n",
      "\n",
      "Modifying Knowledge:  Family consists of 6 people  Category.Family_Attribute Action.Create\n",
      "Modifying Knowledge:  Wife doesn't eat meat  Category.Food_Dislike Action.Create\n",
      "Modifying Knowledge:  Youngest daughter is allergic to dairy  Category.Food_Allergy Action.Create\n",
      "Output from node 'action':\n",
      "---\n",
      "{'messages': [HumanMessage(content=\"There are 6 people in my family. My wife doesn't eat meat and my youngest daughter is allergic to dairy.\"), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_N3OUsTB9UNxI6JF2A6AggH4z', 'function': {'arguments': '{\"knowledge\": \"Family consists of 6 people\", \"category\": \"Attribute\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}, {'index': 1, 'id': 'call_UJz5NaecIpxiwAwlODdUgabA', 'function': {'arguments': '{\"knowledge\": \"Wife doesn\\'t eat meat\", \"category\": \"Dislike\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}, {'index': 2, 'id': 'call_6fnDlpEdBFTgCPBwFISZdXDY', 'function': {'arguments': '{\"knowledge\": \"Youngest daughter is allergic to dairy\", \"category\": \"Allergy\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}]}), ToolMessage(content='Modified Knowledge', tool_call_id='call_N3OUsTB9UNxI6JF2A6AggH4z', name='Knowledge_Modifier'), ToolMessage(content='Modified Knowledge', tool_call_id='call_UJz5NaecIpxiwAwlODdUgabA', name='Knowledge_Modifier'), ToolMessage(content='Modified Knowledge', tool_call_id='call_6fnDlpEdBFTgCPBwFISZdXDY', name='Knowledge_Modifier')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node '__end__':\n",
      "---\n",
      "{'messages': [HumanMessage(content=\"There are 6 people in my family. My wife doesn't eat meat and my youngest daughter is allergic to dairy.\"), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_N3OUsTB9UNxI6JF2A6AggH4z', 'function': {'arguments': '{\"knowledge\": \"Family consists of 6 people\", \"category\": \"Attribute\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}, {'index': 1, 'id': 'call_UJz5NaecIpxiwAwlODdUgabA', 'function': {'arguments': '{\"knowledge\": \"Wife doesn\\'t eat meat\", \"category\": \"Dislike\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}, {'index': 2, 'id': 'call_6fnDlpEdBFTgCPBwFISZdXDY', 'function': {'arguments': '{\"knowledge\": \"Youngest daughter is allergic to dairy\", \"category\": \"Allergy\", \"action\": \"Create\"}', 'name': 'Knowledge_Modifier'}, 'type': 'function'}]}), ToolMessage(content='Modified Knowledge', tool_call_id='call_N3OUsTB9UNxI6JF2A6AggH4z', name='Knowledge_Modifier'), ToolMessage(content='Modified Knowledge', tool_call_id='call_UJz5NaecIpxiwAwlODdUgabA', name='Knowledge_Modifier'), ToolMessage(content='Modified Knowledge', tool_call_id='call_6fnDlpEdBFTgCPBwFISZdXDY', name='Knowledge_Modifier')], 'memories': None, 'contains_information': 'yes'}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.with_config({\"run_name\": \"Memory\"}).stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
